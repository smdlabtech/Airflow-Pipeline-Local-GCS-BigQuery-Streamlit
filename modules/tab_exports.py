# modules/tab_exports.py
import streamlit as st
from utils.gcp_bq import run_query_to_df, extract_table_to_gcs
import pandas as pd
import io

def render():
    st.subheader("üì§ Exports")
    p = st.session_state.get("gcp_project_id"); d = st.session_state.get("bq_dataset")
    if not p or not d:
        st.warning("Compl√®te **GCP Setup**.")
        return

    st.markdown("**1) T√©l√©charger en CSV depuis une requ√™te BigQuery**")
    sql = st.text_area("SQL pour export CSV", value=f"SELECT * FROM `{p}.{d}.dim_customer` LIMIT 1000", height=160)
    if st.button("‚¨áÔ∏è Ex√©cuter & T√©l√©charger CSV"):
        df = run_query_to_df(sql, project=p)
        if df is not None:
            csv = df.to_csv(index=False).encode("utf-8")
            st.download_button("T√©l√©charger result.csv", data=csv, file_name="result.csv", mime="text/csv")
        else:
            st.info("Aucun r√©sultat.")

    st.markdown("---")
    st.markdown("**2) Exporter une table BigQuery vers GCS (CSV/Parquet)**")
    table = st.text_input("Table compl√®te (project.dataset.table)", value=f"{p}.{d}.fact_orders")
    fmt = st.selectbox("Format", ["CSV","PARQUET"])
    gcs_uri = st.text_input("URI GCS cible (ex: gs://my-bucket/exports/fact_orders-*.parquet)", value=f"gs://{st.session_state.get('gcs_bucket','bucket')}/exports/fact_orders-*.parquet")
    if st.button("üöÄ Lancer extract job ‚Üí GCS"):
        job = extract_table_to_gcs(table=table, destination_uri=gcs_uri, fmt=fmt)
        st.success(f"‚úÖ Export lanc√© / termin√©: {job.job_id}")
        st.write("Destination:", gcs_uri)
