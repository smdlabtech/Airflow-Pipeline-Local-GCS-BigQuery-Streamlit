# â˜ï¸ GCP Pipeline v2 â€“ Local â†’ GCS â†’ BigQuery â†’ Streamlit

**NouveautÃ©s** : DimDate, SCD2, onglet Exports (BQâ†’GCS & CSV), squelette **dbt**, **Dockerfile** & **docker-compose**, export **ZIP** depuis l'app.

## ğŸš€ DÃ©marrage local
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

gcloud auth application-default login
gcloud config set project <PROJECT_ID>

streamlit run app.py
```

## ğŸ§± Onglets
1. **GCP Setup** â€“ Project/Bucket/Dataset/Location
2. **Upload â†’ GCS** â€“ envoie `landing/*.csv` dans GCS
3. **Load â†’ BQ** â€“ crÃ©e des tables `staging_*`
4. **SQL Transforms** â€“ crÃ©e `clean_*` (+ code DimDate/SCD2 fourni)
5. **DW Modeling** â€“ boutons : `dim_customer`, `fact_orders`, `dim_date`, `SCD2`
6. **Visualisation** â€“ chart depuis BQ
7. **Exports** â€“ CSV download (requÃªte) & extract job BQâ†’GCS (CSV/Parquet)
8. **Orchestration** â€“ DAG Airflow exemple
9. **dbt** â€“ squelette (staging/warehouse/marts)
10. **Export Projet** â€“ crÃ©ation dâ€™un ZIP complet du repo

## ğŸ³ Docker
```bash
cd docker
docker compose up --build
```
Montez vos creds GCP dans `.gcp/sa.json` si vous utilisez un **service account**.

## ğŸ§ª dbt (optionnel)
```bash
pip install dbt-bigquery
cp dbt/profiles.example.yml ~/.dbt/profiles.yml
dbt debug && dbt run && dbt test
```

## âš ï¸ Droits requis
- GCS: Storage Object Admin (Ã  restreindre finement en prod)
- BQ: BigQuery Data Editor / Job User, etc.


---
### Defaults utilisÃ©s pour ce template
- PROJECT_ID: `bq-small-corp`
- REGION (GCS): `europe-west1`
- BigQuery LOCATION: `EU`
- GCS BUCKET par dÃ©faut: `bq-small-corp-data`
- BigQuery DATASET par dÃ©faut: `demo_dw`
