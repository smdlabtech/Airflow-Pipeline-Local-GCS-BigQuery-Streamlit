# â˜ï¸ GCP Pipeline : Local Data â†’ GCS â†’ BigQuery â†’ DBT â†’ Streamlit

ğŸ‰**NouveautÃ©s** : DimDate, SCD2, onglet Exports (BQâ†’GCS & CSV), squelette **dbt**, **Dockerfile** & **docker-compose**, export **ZIP** depuis l'app.

## ğŸš€ DÃ©marrage local
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

### ğŸ”‘ Authentification GCP (Option A â€“ Application Default Credentials)
Lâ€™application utilise les **Application Default Credentials (ADC)** de GCP.  
Configure-les une fois avec le SDK Google Cloud :

```bash
# Connecter ton compte Google et crÃ©er les credentials ADC
gcloud auth application-default login

# DÃ©finir ton projet par dÃ©faut
gcloud config set project bq-small-corp

# (Optionnel) VÃ©rifier que tout est bien configurÃ©
gcloud auth application-default print-access-token
gcloud config get-value project
```

Ces credentials sont stockÃ©s automatiquement (Windows : `%APPDATA%\gcloud\application_default_credentials.json`)  
et seront utilisÃ©s par la librairie `google-cloud-bigquery` et `google-cloud-storage`.

---


### âš™ï¸ Commandes optionnelles de dÃ©pannage

#### ğŸŸ¢ Solution 2 : VÃ©rifier le projet GCP
```bash
# VÃ©rifiez le projet configurÃ©
gcloud config get-value project

# Si nÃ©cessaire, changez le projet
gcloud config set project VOTRE_PROJECT_ID
```

#### ğŸŸ¢ Solution 3 : VÃ©rifier les permissions
Assurez-vous que :
- Le projet GCP existe  
- Vous avez les droits **BigQuery Admin** et **Storage Admin**  
- Votre compte a les permissions nÃ©cessaires  

#### ğŸŸ¢ Solution 4 : Utiliser un service account (production)
Pour la production, utilisez un fichier de service account :

```python
from google.oauth2 import service_account
from google.cloud import bigquery

credentials = service_account.Credentials.from_service_account_file(
    'chemin/vers/votre/service-account-key.json',
    scopes=["https://www.googleapis.com/auth/cloud-platform"]
)
client = bigquery.Client(credentials=credentials, project=project_id)
```

ğŸ“‹ **Ã‰tapes de rÃ©solution recommandÃ©es**
```bash
# 1) Authentification locale
gcloud auth application-default login

# 2) VÃ©rifier lâ€™authentification
gcloud auth list

# 3) Configurer le projet par dÃ©faut
gcloud config set project bq-small-corp

# 4) Relancer l'application
streamlit run app.py
```

ğŸ”§ **Pour le dÃ©ploiement Docker**
Si vous utilisez Docker, montez vos credentials :
```yaml
# docker-compose.yml
volumes:
  - ~/.config/gcloud:/root/.config/gcloud  # Pour les credentials ADC
  # ou
  - ./service-account-key.json:/app/service-account-key.json  # Pour un service account
```

ğŸ’¡ **Bonnes pratiques**
- DÃ©veloppement local : utilisez `gcloud auth application-default login`  
- Environnements conteneurisÃ©s : utilisez des service accounts  
- Production : privilÃ©giez **Workload Identity Federation** ou des secrets managÃ©s  

â„¹ï¸ Cette erreur est normale lors de la premiÃ¨re configuration d'une application GCP.  
Une fois l'authentification configurÃ©e, elle disparaÃ®t. âœ…

---

### â–¶ï¸ Lancer l'application
```bash
streamlit run app.py
```

## ğŸ§± Onglets
1. **GCP Setup** â€“ Project/Bucket/Dataset/Location
2. **Upload â†’ GCS** â€“ envoie `landing/*.csv` dans GCS
3. **Load â†’ BQ** â€“ crÃ©e des tables `staging_*`
4. **SQL Transforms** â€“ crÃ©e `clean_*` (+ code DimDate/SCD2 fourni)
5. **DW Modeling** â€“ boutons : `dim_customer`, `fact_orders`, `dim_date`, `SCD2`
6. **Visualisation** â€“ chart depuis BQ
7. **Exports** â€“ CSV download (requÃªte) & extract job BQâ†’GCS (CSV/Parquet)
8. **Orchestration** â€“ DAG Airflow exemple
9. **dbt** â€“ squelette (staging/warehouse/marts)
10. **Export Projet** â€“ crÃ©ation dâ€™un ZIP complet du repo

## ğŸ³ Docker
```bash
cd docker
docker compose up --build
```
Montez vos creds GCP dans `.gcp/sa.json` si vous utilisez un **service account**.

## ğŸ§ª dbt (optionnel)
```bash
pip install dbt-bigquery
cp dbt/profiles.example.yml ~/.dbt/profiles.yml
dbt debug && dbt run && dbt test
```

## âš ï¸ Droits requis
- GCS: Storage Object Admin (Ã  restreindre finement en prod)
- BQ: BigQuery Data Editor / Job User, etc.

### Defaults utilisÃ©s pour ce template
- PROJECT_ID: `bq-small-corp`
- REGION (GCS): `europe-west1`
- BigQuery LOCATION: `EU`
- GCS BUCKET par dÃ©faut: `bq-small-corp-data`
- BigQuery DATASET par dÃ©faut: `demo_dw`
